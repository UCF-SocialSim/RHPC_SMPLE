%!TEX root =  RHPC_SMPLE_UsersManual.tex

\chapter{Parallelization} \label{chap:Parallelization}
\textbf{Parallelization} is the division of a single computational problem across multiple processors. The operation can be anything from a simple but very large mathematical operation to a complex simulation. The goal of parallelization is to reduce the real-world time of solving the problem the simulation is meant to address. In the ideal case, the time to solve the problem is changed from $T$ to $\frac{T}{N}$, where $N$ is the number of processors across which the problem is divided. For example, if it takes 10 minutes to add a billion numbers together on one processor, then if the problem is divided across 10 processors, it could theoretically be solved in one minute.

In practice, the performance gain is never this perfect: although the computational time might be reduced linearly based on the number of processors, the act of parallelization imposes some additional costs. In the case of the simple addition problem above, we can assume that the billion numbers are stored on one of the processors at the start, and that this same processor is the one that needs the sum at the end. In this case, the overhead comes because the process needs to divide up and distribute the data across the ten processes (presumably itself and the other nine processes), and then receive the sums back from the other nine processes, and finally perform the last addition in which those sums are added together to get the final result.

However, this idealized case is very far from the situation that occurs when parallelizing an agent-based model. In the case of the simple addition problem, each of the processes can act independently because the result that would be obtained on one process does not depend on the results from any other process. Consequently, each process can perform the operation without any interaction with the other processes. However, in an agent-based model, where the critical aspect of the simulation being tested is the way that the agents interact, the potential for cross-process dependencies is very high.

A common example is agents positioned on a grid and moving such that each agent takes a turn in which it chooses a direction and moves some number of grid squares. A very common corollary in this kind of simulation is that a grid square can be occupied only by one agent at a time. Consequently, the requirement that the agents take turns is crucial: if only one agent is acting at a time, then it can assess which squares around it are empty and move into any empty square without violating the rule.

This simple state of affairs becomes significantly more complicated when the simulation is parallelized and agents are acting on more than one process simultaneously. If this is the case, it is possible for an agent to be unsure if a particular grid square is empty or not: it might be possible for an agent on another process to also be considering that square as a destination, or even to have moved there already. If the information that the square is occupied has not traveled from one process to another, the agent on the second process may not know that the agent from the first process has moved and the square is now taken.

In the worst-case scenario, this would create a chain of cascading dependencies: an agent would poll another process and/or negotiate with another process to determine which agent would act first, but all other agents with actions that might be impacted by this negotiation might also have to wait for it to be resolved. This could involve immediately neighboring agents waiting for their opportunities to move, which would cause their neighbors a few steps away to also have to wait, etc. It is even possible that the only resolution to this would be to impose a global control mechanism that would designate one agent across processes and allow that agent to act, effectively eliminating parallelization and its advantages. \footnote{In other contexts, solutions to this problem can reach extraordinary lengths: For example, `optimistic' approaches would have the agent move into the square and would merely make a note that it could be occupied. They would then continue allowing all other agents on that process to move. If information was later received that the square was occupied and the move into it was invalid, the entire simulation is then `rolled back' to the point where the collision occurred. The cost of the rollback can be high, but if it is rare and the benefit of moving ahead is great, then this strategy might be efficient.}

Because there is a need to establish some reconciliation process for cases like this, parallelizing ABMs is non-trivial. The main question to be faced is whether the reconciliation process imposed as a necessity to perform parallel operations is one that keeps the outcome of the simulation valid---that is, that parallelization does not make the simulation produce results that are not usable. In fact, of course, the strategy for implementing agent behaviors on a single process must also address this question, because the real world is parallel but the simulation is not; parallelization does not change this, but it does throw the question into the spotlight, because many tried-and-true strategies in serial environments (e.g., agents taking turns) do not work in parallel ones.

\subsection{When not to parallelize}

Parallelization of agent-based models faces an additional practical challenge. It is very good to take the simulation run time and reduce it by some factor, and if that factor approaches $\frac{1}{N}$ for $N$ processes, this can be an important gain. However, agent-based models typically rely on stochastic repetitions of runs so that the simulation is not run once, but multiple times, creating a range of results and a measurable distribution. Moreover, these are often conducted hand-in-hand with tests that involve multiple varying parameters. Consequently, the simulation must be run some number of times. The issue can be seen when assuming that this number is also $N$: if there are $N$ processors, the simulation can be made to run in $\frac{1}{N}$ time by using all of the processors. 
However, if needing to then run $N$ repetitions, it will take just as long to complete all of these as it would if all these runs were started as independent processes run simultaneously on all available processors. In general, the number of runs needed is much larger than the number of processors available, and so the overhead of running the simulation in parallel---as well as the extra development time to parallelize it---is better off avoided, and the runs should simply be done as independent, single-process runs.

There is, however, one exception to this: parallelizing a simulation also allows it to be divided up in \textit{size} as well as time. If a simulation is too large to be run on the memory available to a single process, then parallelizing it may be a solution. In the not-too-distant past, processes with 16GB, or even 8GB were the norm on even very large `supercomputers', and consequently a simulation that exceeded this, which was not uncommon, would need to be run on more than one process. Today, however, processes are available that have hundreds of GB of memory, enough to accommodate very large ABMs.

\section{Synchronization}
As discussed in the previous section, parallelization involves processes acting independently. As one process modifies the state of the simulation (e.g., by allowing an agent to move), other processes are temporarily unaware of this modification. This is what causes the difficulty with dependencies discussed above. The delay in updating the information from one process to another is termed being out of synchronization. It is intentional---it is what allows the processes to work independently\footnote{The general paradigm here is processes that do not share memory---in some systems, separate processes can share memory and this would seem to allow all processes to have access to the up-to-date simulation information. However, there could still be collisions and problems. For example, one process reads that the grid square is `empty' and then begins to perform the operation of letting another agent decide whether to occupy it, and while this is underway, another process moves an agent into that square. Direct access to memory does not resolve the basic issue, which derives from the parallelization.}. However, it raises an important design consideration: when does the simulation need to share information back across the processes?

This could be done on an ad hoc basis in some cases, but generally the assumption is that the processes will be allowed to run independently and then a global synchronization operation will be performed. This is the paradigm supported in the Repast HPC toolkit.

However, there are three \textit{different} kinds of synchronization that can be performed. They derive from the underlying paradigm that Repast HPC uses for parallelism. There are two crucial components to this paradigm:

First, the simulation tracks two different kinds of information about the agents: agent \textit{state} refers to the internal variables that an agent has (like salary, fuel, emotional level, etc.), and is internal to each agent; agent \textit{relationships} are tracked separately. Relationships can be of two kinds: networks, in which any agent can have a relationship with any other agents, and locations in a cartesian space. Location in space might initially seem to be an attribute of the agent, but in fact the location of an agent in absolute terms (e.g., at grid point $x=390, y=257$) only has meaning with respect to other things in the simulation\footnote{This is akin to the ability to choose different frames of reference; one can choose any point in the space and call it x=0, y=0, and this should not change the outcome of the simulation.}.

Second, the simulation is aware that agents exist on specific processes; agent 1 may have its `home' on Process 0. However, if agent 1 has a relationship with an agent on another process- for example, being close to it in space or being linked to it in a network- then Repast HPC will provide that process a copy of agent 1. This will allow the other agent to interact with agent 1. This copy is referred to as a \textit{non-local} agent, or sometimes (informally) a \textit{ghost agent}. The fact that an agent is local to a specific process is called that agent's \textit{status}. All relationships are kept in structures that are termed \textit{projections}.

This leads to the three kinds of synchronization:

\begin{itemize}
\item \textbf{Synchronization of Agent State} means updating the internal states of all non-local agents
\item \textbf{Synchronization of Projections} means updating the relationships an agent is involved in, including movement in space.
\item \textbf{Synchronization of Agent Status} means updating the status of an agent.
\end{itemize}

The first of these is the simplest: synchronizing agent states means copying the up-to-date information from the agents' home processes into the copies on other processes. Suppose Agent 1 is local to Process 0, and initially has an account balance of \$100. A copy is needed on Process 1, and this copy also initially has an account balance of \$100. But, during a time tick, the agent wins the lottery, and its account balance goes to \$30,000. For a period of time, the agent locally has the higher balance, but the non-local copy does not. Synchronizing the agent state copies the new balance into the copy of the agent on Process 1.

While this is the simplest kind of synchronization, it also imposes the hardest constraint: \textit{Information flows from home processes to non-local processes, but \textbf{never the other direction}}. Because of this, no change to the account balance of the `ghost' copy of Agent 0 on Process 1 should ever be made. The rationale for this is simple: the same agent may be copied as a `ghost' to multiple other processes, and updates made to all of these can't be reconciled. If something were happening on Process 1 that would affect the account balance, the correct way to do this would be to bring the information from Process 1 (including any other agents) back to Process 0, and have the interaction that makes the change take place there.

The synchronization of projections is more complicated, but a simple example can convey the basics. Supposed an agent is in a simulation with a grid space, and is near the edge of the space that the local process controls (say, the agent is at x = 99, with the boundary at x = 100). The adjacent process has the ability to perceive 3 grid spaces into this one; hence this agent is visible to the other process, where it appears as a non-local agent at x = 99. During the tick, this agent moves back a step, to x = 98. The copy on the other process remains at x = 99 until projection information is synchronized. When projection information is synchronized, the agent's new position at x = 98 is available to the other process via the non-local copy.

Note that these two operations can be done independently: the position can be changed without updating the account balance (internal state). Repast HPC does not require that they both be performed. Because these are computationally intensive operations, you can choose to do one without doing the other. If, for example, you know that account balances have been updated but no positions have been changed, then there is no need to update both elements.

Synchronization of agent status is more challenging because it has more possible variations. Two common ones are:

\begin{itemize}
\item A case in which an agent was local to Process 0 and had a non-local copy on Process 1, but which for some reason (for example, moving from x = 99 to x = 101, thus crossing the boundary into the space controlled by the adjacent process) must be moved so that it becomes local on Process 1. In this case, the original on Process 0 might become a `ghost' copy, while the copy on Process 1 becomes a full agent.
\item A case in which an agent local to Process 0, with non-local copies on other processes, is removed entirely from the simulation (e.g., `dies'). While the simulation is out of synchronization, the non-local copies persist; once agent status is synchronized, the non-local copies are all removed.
\end{itemize}

\section {Building and Running Parallel Simulations in RHPC SMPL}